
# Домашнее задание к занятию "13.Системы мониторинга"

## Обязательные задания

1. Вас пригласили настроить мониторинг на проект. На онбординге вам рассказали, что проект представляет из себя 
платформу для вычислений с выдачей текстовых отчетов, которые сохраняются на диск. Взаимодействие с платформой 
осуществляется по протоколу http. Также вам отметили, что вычисления загружают ЦПУ. Какой минимальный набор метрик вы
выведите в мониторинг и почему?

<details>  
<summary> Ответ на вопрос №1 </summary>  

Я бы выделил **три уровня метрик** и собрал бы следующий минимальный набор, ориентируясь на **Four Golden Signals** (Latency, Traffic, Errors, Saturation).

#### Уровень 1: Бизнес-логика (Самое важное для пользователя)

Эти метрики отвечают на вопрос "Работает ли сервис так, как нужно конечному пользователю?".

1.  **Скорость формирования отчетов (Latency):**
    *   **Что:** `http_request_duration_seconds` (гистограмма). Замеряем время от получения HTTP-запроса до момента отправки ответа клиенту.
    *   **Почему:** Пользователь напрямую ощущает эту задержку. Высокое время выполнения — главный признак проблем.
    *   **Детали:** Обязательно нужно измерять квантили (p50, p95, p99). p99 покажет наихудший опыт самых "невезучих" пользователей.

2.  **Количество запросов (Traffic):**
    *   **Что:** `http_requests_total` (счетчик). Общее количество входящих HTTP-запросов.
    *   **Почему:** Показывает нагрузку на сервис. Резкий рост или падение (до 0) — critical-алерт.

3.  **Количество успешных и неуспешных операций (Errors):**
    *   **Что:** `http_requests_total{status=~"5.."}` (счетчик) и `http_requests_total{status=~"2.."}`. Считаем ответы с кодами 5xx (ошибки сервера) и 2xx (успех).
    *   **Почему:** Прямой индикатор доступности сервиса. Рост 5xx- ошибок означает, что пользователи не могут получить свои отчеты.
    *   **Дополнительно:** Можно добавить счетчик 4xx-ошибок для отслеживания проблем с клиентскими запросами.

#### Уровень 2: Инфраструктура и ресурсы

Эти метрики отвечают на вопрос "Почему сервис работает медленно или с ошибками?".

1.  **Загрузка CPU (Saturation):**
    *   **Что:** `node_cpu_usage` или `process_cpu_seconds_total`. Загрузка процессора в процентах (на уровне всей виртуалки/контейнера) или конкретного процесса.
    *   **Почему:** Прямо указано, что вычисления нагружают ЦПУ. Это ключевой ресурс для вашего сервиса. Высокая загрузка (например, >80% продолжительное время) — признак необходимости масштабирования или оптимизации кода.

2.  **Дисковое I/O и свободное место (Saturation):**
    *   **Что:**
        *   `node_disk_io_time_seconds` (загрузка диска).
        *   `node_filesystem_avail_bytes` (свободное место на файловой системе).
    *   **Почему:** Отчеты сохраняются на диск. Если диск медленный или перегружен операциями записи, это будет напрямую влиять на общую latency. Заканчивающееся место — гарантированный даунтайм в будущем.

3.  **Потребление памяти (Saturation):**
    *   **Что:** `node_memory_MemAvailable_bytes` или `process_resident_memory_bytes`.
    *   **Почему:** Хоть основная нагрузка и на ЦПУ, вычисления часто используют оперативную память для хранения промежуточных данных. Нехватка памяти приведет к свопу (и жутким тормозам) или OOM-Kill процесса.

#### Уровень 3: Синтетические метрики (Опционально, но сильно повышает качество)

*   **Что:** Наличие простого скрипта (через Blackbox exporter или Synthetics), который периодически (например, раз в минуту) отправляет тестовый запрос на генерацию отчета и проверяет, что ответ приходит с кодом 200 и за приемлемое время.
*   **Почему:** Это самый надежный способ проверить доступность сервиса *с точки зрения пользователя*, а не просто факт работы процесса. Если процесс "висит", но не отвечает, метрики уровня 2 будут в норме, а этот тест упадет.

---

### Итоговый минимальный набор метрик для дашборды и алертинга:

| Метрика                              | Тип         | Зачем?                                                                |
| :----------------------------------- | :---------- | :-------------------------------------------------------------------- |
| **HTTP Request Duration (p95, p99)** | Гистограмма | **Основной индикатор производительности.**                            |
| **HTTP Request Rate**                | Счетчик     | **Понимание нагрузки.** Резкое падение = нет трафика или сервис умер. |
| **HTTP 5xx Error Rate**              | Счетчик     | **Аварийный алерт.** Пользователи не могут получить отчет.            |
| **CPU Utilization**                  | Gauge       | **Главный ресурсный ограничитель.** Рост ведет к росту latency.       |
| **Disk Space Available**             | Gauge       | **Аварийный алерт.** Закончится место — сервис упадет.                |
| **Disk I/O Utilization**             | Gauge       | **Вторичный фактор, влияющий на latency.**                            |
| **Memory Available**                 | Gauge       | **Ресурсное ограничение.**                                            |

**Почему такой набор минимальный и достаточный?**

Он покрывает все золотые сигналы и позволяет ответить на ключевые вопросы:
1.  **Доступен ли сервис для пользователя?** (Traffic, Errors)
2.  **Быстро ли он работает?** (Latency)
3.  **Есть ли риск сломаться из-за нехватки ресурсов?** (Saturation: CPU, Disk, Memory)
4.  **В чем наиболее вероятная причина проблемы?** (Совместный анализ: если latency растет вместе с CPU -> проблема в вычислениях; если latency растет, а CPU нет -> возможно, проблема с диском).

Это основа, от которой можно будет отталкиваться, добавляя более детальные метрики (например, потребление CPU конкретными endpoint'ами, количество сгенерированных отчетов и т.д.).
</details>  

#
2. Менеджер продукта посмотрев на ваши метрики сказал, что ему непонятно что такое RAM/inodes/CPUla. Также он сказал, 
что хочет понимать, насколько мы выполняем свои обязанности перед клиентами и какое качество обслуживания. Что вы 
можете ему предложить?  
<details>  
<summary> Ответ на вопрос №2 </summary>  
  
Я бы предложил перевести технические метрики на язык бизнеса, введя понятие **SLA (Service Level Agreement) / SLO (Service Level Objectives)**.

Вот что я предлагаю донести до менеджера и вывести на отдельный, понятный дашборд:

#### 1. Вместо "Непонятных RAM/CPU/Дисков" — Вводим Здоровье Системы (System Health Score)

Это высокоуровневый агрегированный показатель, который говорит "все хорошо" (зеленый) или "есть проблема" (желтый/красный). Его можно реализовать как статусную панель или "светофор".

**Пример:**
*   **ЗЕЛЕНЫЙ (Все отлично):** > 95% запросов обрабатываются успешно и быстрее 10 секунд.
*   **ЖЕЛТЫЙ (Деградация сервиса):** >5% запросов обрабатываются дольше 10 секунд или есть ошибки.
*   **КРАСНЫЙ (Сервис недоступен/критически медленный):** >10% ошибок или среднее время ответа > 30 секунд.

Цифры (10 сек, 5%) — это пример. Их нужно обсудить и зафиксировать как целевые показатели (SLO).

#### 2. Ключевые Бизнес-Метрики Качества Обслуживания (SLOs)

Вместо технических терминов, мы говорим на языке бизнеса. Вот что действительно важно для клиента:

| Что важно клиенту                    | Как это измерить (Метрика)                                                                                              | Как представить менеджеру                                                                                                          |
| :----------------------------------- | :---------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------- |
| **1. Сервис доступен и отвечает.**   | **Availability (Доступность)** <br> `(Всего запросов - Запросы с ошибкой 5xx) / Всего запросов * 100%`                  | **"Доступность сервиса: 99.95%"** <br> Цель: > 99.9% в месяц. Простой и понятный процент успешных запросов.                        |
| **2. Отчеты формируются БЫСТРО.**    | **Latency (Скорость ответа)** <br> Процент запросов, которые выполнились быстрее целевого времени (например, 5 секунд). | **"Скорость генерации отчетов: 98% отчетов создаются менее чем за 5 секунд"** <br> Это гораздо понятнее, чем "p95 latency = 4.3s". |
| **3. Отчеты формируются КОРРЕКТНО.** | **Error Rate (Частота ошибок)** <br> `(Количество запросов с ошибкой 5xx / Всего запросов) * 100%`                      | **"Частота ошибок: 0.1%"** <br> Показывает, как часто у клиентов что-то ломается.                                                  |
| **4. Сервис не перегружен.**         | **Throughput (Производительность)** <br> `Количество успешно сгенерированных отчетов в минуту/час`                      | **"Мы стабильно обрабатываем ~100 отчетов в час"** <br> Показывает мощность системы и помогает планировать нагрузку.               |

#### 3. Как это визуализировать: Дашборд для Менеджера Продукта

Нужен простой и наглядный дашборд, который можно понять за 5 секунд.

**Верхняя панель:**
*   **Большие цифры (KPI):**
    *   `Доступность за сегодня: 99.98%`
    *   `Среднее время ответа: 1.2 сек`
    *   `Отчетов создано сегодня: 12,458`
*   **Светофор/статус:** **ЗЕЛЕНЫЙ** (Система работает в рамках SLO)

**Графики ниже:**
1.  **График доступности за последнюю неделю.** (Линия, которая должна быть близка к 100%).
2.  **График скорости.** Две линии: "Среднее время ответа" и "Целевое время (SLO)", например, 5 сек. Наглядно видно, когда мы были хуже цели.
3.  **График количества ошибок в час.** Столбчатая диаграмма. Показывает, в какие часы были проблемы.
4.  **График нагрузки (количество успешных запросов в час).** Показывает пиковое время и общую загруженность.

---

#### Итог: Что предложить менеджеру

1.  **Перейти от технических метрик к бизнес-ориентированным SLO.**
2.  **Определить целевые цифры (SLO) вместе с ним:** "Какую скорость формирования отчета мы считаем приемлемой для клиента? 5 секунд? 10? Какой процент доступности мы гарантируем? 99.9%?"
3.  **Создать простой дашборд** с ключевыми показателями: **Доступность, Скорость, Качество (отсутствие ошибок), Нагрузка.**
4.  **Внедрить механизм оповещения** не о том, что "CPU загружен на 95%", а о том, что "Мы нарушили SLO по скорости ответа для 5% наших пользователей". Это *алерт, ориентированный на пользователя*.

Такой подход переводит диалог с технического "у нас все хорошо, потому что графики в норме" на деловой "мы выполняем свои обязательства перед клиентами на 99.9%, вот отчет".
</details>  

#
3. Вашей DevOps команде в этом году не выделили финансирование на построение системы сбора логов. Разработчики в свою 
очередь хотят видеть все ошибки, которые выдают их приложения. Какое решение вы можете предпринять в этой ситуации, 
чтобы разработчики получали ошибки приложения?  
<details>  
<summary> Ответ на вопрос №3 </summary>  

#### Рекомендуемый план действий
Необходимо настроить сбор и анализ логов на open source решениях.
1.  **Быстрое решение на ближайшие пару недель:** Реализовать **bash-скрипт с webhook** для отправки ошибок в Slack/Telegram. Это даст разработчикам мгновенную обратную связь и займет минимум времени.
2.  **Среднесрочное и самое эффективное решение:** **Интегрировать Sentry** по бесплатному плану. Это даст разработчикам максимальное качество информации об ошибках с минимальными затратами на внедрение и поддержку.
3.  **Как только финансирование появится:** Перейти на платный план Sentry или развернуть собственный стек (Loki, Elasticsearch) для полного сбора всех логов, а не только ошибок.

**Главный посыл:** Нельзя собирать все логи -> фокусируемся только на самом важном — на **ошибках (ERROR/Exception)**. И доставляем их разработчикам максимально простым и дешевым способом.
</details>

#
4. Вы, как опытный SRE, сделали мониторинг, куда вывели отображения выполнения SLA=99% по http кодам ответов. 
Вычисляете этот параметр по следующей формуле: summ_2xx_requests/summ_all_requests. Данный параметр не поднимается выше 
70%, но при этом в вашей системе нет кодов ответа 5xx и 4xx. Где у вас ошибка?  
<details>  
<summary> Ответ на вопрос №4 </summary>  
Почему формула не работает и что учитывается в знаменателе?
Формула интерпретирует любой запрос, который не вернул код 2xx, как "неуспешный" с точки зрения SLA. Но в реальности:

summ_all_requests включает в себя:

2xx (Успех) -> Учитываются как "успех" в вашей формуле
3xx (Перенаправление) -> Учитываются как "провал" в вашей формуле
4xx (Ошибка клиента) -> У вас нет, но если бы были, тоже считались бы "провалом"
5xx (Ошибка сервера) -> У вас нет, и это правильно, что они считались бы "провалом"

Проблема: Запросы с кодом 3xx (например, 301 Moved Permanently, 302 Found, 304 Not Modified) — это НЕ ошибки. Это часть нормального workflow приложения.
304 Not Modified — это корректный ответ кеширующему клиенту, который экономит трафик и ресурсы.
301/302 — это стандартный способ перенаправить пользователя с HTTP на HTTPS, с одного URL на другой и т.д.  
  
**Правильная формула для SLA по доступности:**
```
SLA = (1 - (summ_5xx_requests + summ_4xx_requests_which_are_our_fault) / summ_all_requests) * 100%
```
</details>  

#
5. Опишите основные плюсы и минусы pull и push систем мониторинга.
<details>  
<summary> Ответ на вопрос №5 </summary>  

---

### Push-модель (Толкающая)

**Как работает:** Агенты на monitored-хостах самостоятельно инициируют соединение и отправляют (push) метрики на центральный сервер сбора.

**Аналогия:** Почтовое отделение (агент) само привозит посылки (метрики) на сортировочный центр (сервер).

#### Плюсы (+)

1.  **Гибкость и мобильность:** Легко отправить метрики с любого устройства, из любой сети, главное — чтобы был доступ до сервера приёмника. Идеально для:
    *   Клиентских устройств (браузеры, мобильные приложения).
    *   Кратковременных задач (AWS Lambda, batch-задачи), которые живут недолго и не могут ждать, пока их "спросят".
    *   Ситуаций с динамической инфраструктурой (автоскейлинг), где набор целей для опроса постоянно меняется.

2.  **Мгновенность:** Метрики отправляются сразу после их сбора или по истечении интервала. Сервер всегда получает данные с минимальной задержкой.

3.  **Контроль нагрузки на клиенте:** Клиент сам решает, когда и сколько данных отправлять. Это может помочь сгладить пики потребления ресурсов на стороне агента.

4.  **Проще в настройке сети (с точки зрения клиента):** Достаточно разрешить только **исходящий трафик** с клиентских машин на один порт сервера. Не нужно давать серверу доступ до всей инфраструктуры.

#### Минусы (-)

1.  **Риск потери данных (Blackholes):** Если сервер сбора перезагрузился или временно недоступен, метрики, отправленные в этот период, **теряются безвозвратно**. Клиент не хранит их.

2.  **Сложность контроля доступа:** Любой, кто знает адрес сервера, может отправить на него метрики. Требуются дополнительные механизмы аутентификации и авторизации (токены, сертификаты), чтобы избежать попадания ложных данных.

3.  **Проблема "тихого" отказа (Silent Failures):** Если агент перестал отправлять метрики, сложно понять почему: то ли он "умер", то ли сломался скрипт, то ли проблемы с сетью. Нет простого способа отличить простое бездействие от сбоя.

4.  **Сложность обнаружения целей:** Серверу нужно заранее знать, от кого он *может* получать данные, или иметь механизм динамической регистрации. Или же клиенты должны сами регистрироваться.

---

### Pull-модель (Тянущая)

**Как работает:** Центральный сервер периодически опрашивает (pull) все известные ему цели (targets), запрашивая у них метрики.

**Аналогия:** Сортировочный центр (сервер) сам отправляет курьеров (запросы) по известным адресам (targets) для забора посылок (метрик).

#### Плюсы (+)

1.  **Высокая надёжность и гарантия доставки:** Сервер сам управляет процессом сбора. Если цель не ответила, он узнает об этом немедленно и может повторить попытку или зафиксировать факт недоступности цели как метрику (`up{job="..."} = 0`). **Данные не теряются** из-за кратковременной недоступности сервера — он просто запросит их снова позже.

2.  **Конфигурация и контроль на стороне сервера:** Легко centrally управлять тем, что и как часто мы собираем. Легко добавить или убрать цель для мониторинга, не внося изменения на самих хостах.

3.  **Безопасность и контроль доступа:** Сервер инициирует соединения только с доверенными целями из своего white-list. Посторонние не могут просто так отправить данные. Это упрощает аутентификацию (часто достаточно TLS).

4.  **Идеально для статической инфраструктуры:** Отлично работает в предсказуемом окружении, где набор серверов известен и меняется нечасто.

#### Минусы (-)

1.  **Сложность в динамических средах:** Для мониторинга кратковременных задач (например, контейнеров в Kubernetes) нужно использовать механизмы服务发现 (service discovery), чтобы автоматически находить и добавлять новые цели для опроса.

2.  **Требует большего доступа в сети:** Сервер должен иметь **сетевой доступ до ВСЕХ monitored-хостов**. Это создает более сложную модель безопасности (DMZ, брандмауэры) по сравнению с Push.

3.  **Риск недосмотреть:** Если сервер не знает о существовании какого-то хоста, он никогда его не опросит. Такие хосты могут оставаться "в тени".

4.  **Задержка данных (Latency):** Данные обновляются только с интервалом опроса (scrape_interval). Между опросами актуальные данные находятся только на самом целевом хосте.

---

### Сводная таблица

| Критерий                   | Push                                    | Pull                             |
| :------------------------- | :-------------------------------------- | :------------------------------- |
| **Инициатор соединения**   | Клиент (Agent)                          | Сервер (Server)                  |
| **Сетевая безопасность**   | Проще (исходящие с клиента)             | Сложнее (входящие на клиента)    |
| **Динамические цели**      | **Отлично** (клиент сам регистрируется) | Сложно (нужен Service Discovery) |
| **Кратковременные задачи** | **Идеально**                            | Затруднено                       |
| **Надёжность доставки**    | Низкая (риск потери)                    | **Высокая**                      |
| **Контроль над сбором**    | На стороне клиента                      | **На стороне сервера**           |
| **Свежесть данных**        | Высокая (отправляются сразу)            | Зависит от интервала опроса      |

### Вывод и современный тренд

Не существует однозначно "правильного" выбора. Часто используется **гибридный подход**:

*   **Prometheus (Pull)** как основной мониторинг инфраструктуры и долгоживущих сервисов за его надежность и контроль.
*   **Pushgateway (для Push)** как дополнение к Prometheus для сбора метрик от кратковременных задач (джобов, скриптов), которые не могут ждать опроса.
*   **StatsD/Graphite (Push)** для высокочастотных данных, таких как инкременты счетчиков (например, количество запросов), где нестрашно потерять несколько значений.

**Тренд:** Современные системы часто абстрагируют этот выбор от пользователя. Агенты (как Datadog Agent или Telegraf) работают в **гибридном режиме**: они сами собирают метрики на хосте (как бы pull изнутри), а затем **push**-ят их на центральный сервер, совмещая преимущества обоих моделей.
</details>

#
6. Какие из ниже перечисленных систем относятся к push модели, а какие к pull? А может есть гибридные?

    - Prometheus 
    - TICK
    - Zabbix
    - VictoriaMetrics
    - Nagios

<details>  
<summary> Ответ на вопрос №6 </summary>  

| Система             | Основная модель | Гибридные возможности                 |
| :------------------ | :-------------- | :------------------------------------ |
| **Prometheus**      | **Pull**        | Push через Pushgateway                |
| **TICK/InfluxDB**   | **Push**        | Telegraf может pull-ить из источников |
| **Zabbix**          | **Гибридная**   | Нативная поддержка и Push, и Pull     |
| **VictoriaMetrics** | **Гибридная**   | Нативно принимает Push и умеет Pull   |
| **Nagios**          | **Pull**        | Push через NSCA/NRDP                  |

**Общий вывод:** Чисто push или чисто pull системы встречаются все реже. Современный тренд — это гибридные решения, которые выбирают модель в зависимости от конкретного сценария сбора данных.
</details>  

#
7. Склонируйте себе [репозиторий](https://github.com/influxdata/sandbox/tree/master) и запустите TICK-стэк, 
используя технологии docker и docker-compose.

В виде решения на это упражнение приведите скриншот веб-интерфейса ПО chronograf (`http://localhost:8888`). 

P.S.: если при запуске некоторые контейнеры будут падать с ошибкой - проставьте им режим `Z`, например
`./data:/var/lib:Z`
#
8. Перейдите в веб-интерфейс Chronograf (http://localhost:8888) и откройте вкладку Data explorer.
        
    - Нажмите на кнопку Add a query
    - Изучите вывод интерфейса и выберите БД telegraf.autogen
    - В `measurments` выберите cpu->host->telegraf-getting-started, а в `fields` выберите usage_system. Внизу появится график утилизации cpu.
    - Вверху вы можете увидеть запрос, аналогичный SQL-синтаксису. Поэкспериментируйте с запросом, попробуйте изменить группировку и интервал наблюдений.

Для выполнения задания приведите скриншот с отображением метрик утилизации cpu из веб-интерфейса.
#
9. Изучите список [telegraf inputs](https://github.com/influxdata/telegraf/tree/master/plugins/inputs). 
Добавьте в конфигурацию telegraf следующий плагин - [docker](https://github.com/influxdata/telegraf/tree/master/plugins/inputs/docker):
```
[[inputs.docker]]
  endpoint = "unix:///var/run/docker.sock"
```

Дополнительно вам может потребоваться донастройка контейнера telegraf в `docker-compose.yml` дополнительного volume и 
режима privileged:
```
  telegraf:
    image: telegraf:1.4.0
    privileged: true
    volumes:
      - ./etc/telegraf.conf:/etc/telegraf/telegraf.conf:Z
      - /var/run/docker.sock:/var/run/docker.sock:Z
    links:
      - influxdb
    ports:
      - "8092:8092/udp"
      - "8094:8094"
      - "8125:8125/udp"
```

После настройке перезапустите telegraf, обновите веб интерфейс и приведите скриншотом список `measurments` в 
веб-интерфейсе базы telegraf.autogen . Там должны появиться метрики, связанные с docker.

